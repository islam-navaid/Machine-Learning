{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tfidf_implentation_Assignment_3",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZJPMlVM3P1t",
        "colab_type": "text"
      },
      "source": [
        "### **TASK 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amGkI_nzRqMe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        },
        "outputId": "ba531ec3-525b-4025-fe7c-04e2893a57af"
      },
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "import math\n",
        "\n",
        "corpus = [\n",
        "     'this is the first document',\n",
        "     'this document is the second document',\n",
        "     'and this is the third one',\n",
        "     'is this the first document',\n",
        "]\n",
        "\n",
        "documentsCount_1 = len(corpus)\n",
        "\n",
        "def fit(dataset):\n",
        "    unique_words = set() \n",
        "    if isinstance(dataset, (list,)):\n",
        "        for row in dataset:                                   \n",
        "            for word in row.split(\" \"):                              \n",
        "                if len(word) < 2:\n",
        "                    continue\n",
        "                unique_words.add(word)\n",
        "        unique_words = sorted(list(unique_words))\n",
        "        vocab = {j:i for i,j in enumerate(unique_words)}\n",
        "        return vocab\n",
        "  \n",
        "vocab = fit (corpus)\n",
        "\n",
        "rows = []\n",
        "columns = []\n",
        "values = []\n",
        "idf_values = {}\n",
        "k = set()\n",
        "\n",
        "def transform(dataset,vocab):\n",
        "    if isinstance(dataset, (list,)):\n",
        "        for idx, row in enumerate(tqdm(dataset)):              \n",
        "            review = dict(Counter(row.split()))    \n",
        "            for word, freq in review.items():                    \n",
        "                if len(word) < 2:\n",
        "                    continue\n",
        "                tf = freq/(len(review))\n",
        "                idf = computeIDF(word,vocab,dataset)\n",
        "                if idf!=0:\n",
        "                   idf_values[word]= idf\n",
        "                tfidf = tf * idf                                                                     \n",
        "                col_index = vocab.get(word, -1)                \n",
        "                if col_index !=-1:                             \n",
        "                   if tfidf!=0:\n",
        "                      rows.append(idx)\n",
        "                      columns.append(col_index)\n",
        "                      values.append(tfidf) \n",
        "        return normalize(csr_matrix((values, (rows,columns)), shape=(len(dataset),len(vocab))))\n",
        "    else:\n",
        "        print(\"you need to pass list of strings\")\n",
        "\n",
        "\n",
        "def computeIDF(word,vocab,dataset):  \n",
        "    for val in k: \n",
        "        if word==val:\n",
        "           return 0\n",
        "    k.add(word)\n",
        "    z = 0 \n",
        "    if isinstance(dataset, (list,)):\n",
        "       for row in dataset:                        \n",
        "           for word1 in row.split(\" \"):        \n",
        "               if word1 == word:\n",
        "                   z = z+1\n",
        "                   break\n",
        "               continue\n",
        "    idf = 1 + math.log((1+documentsCount_1)/(1+z))\n",
        "    return idf\n",
        "\n",
        "y = transform(corpus,vocab)\n",
        "\n",
        "\n",
        "print(\"\\n\\n\\n                                 ----Custom implement tdidfVectorizer Results----\")\n",
        "\n",
        "print(\"\\n*IDF VALUES AND FEATURE NAMES\\n\")\n",
        "for x in idf_values.items(): \n",
        "        print(x, end =' ') \n",
        "        print() \n",
        "print(\"\\n*OUTPUT CORRESPONDING TO FIRST DOCUMENT\\n\")\n",
        "print(y[0].toarray())\n",
        "\n",
        "print(\"\\n\\n\\n                                     -----sklearn tdidfVectorizer Results-----\")\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(corpus)\n",
        "skl_output = vectorizer.transform(corpus)\n",
        "\n",
        "print(\"\\n\\n*IDF VALUES\",vectorizer.idf_)\n",
        "print(\"\\n*FEATURE NAMES\",vectorizer.get_feature_names())\n",
        "print(\"\\n*OUTPUT CORRESPONDING TO FIRST DOCUMENT\\n\")\n",
        "print (skl_output[0].toarray())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 1416.99it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "                                 ----Custom implement tdidfVectorizer Results----\n",
            "\n",
            "*IDF VALUES AND FEATURE NAMES\n",
            "\n",
            "('this', 1.0) \n",
            "('is', 1.0) \n",
            "('the', 1.0) \n",
            "('first', 1.5108256237659907) \n",
            "('document', 1.2231435513142097) \n",
            "('second', 1.916290731874155) \n",
            "('and', 1.916290731874155) \n",
            "('third', 1.916290731874155) \n",
            "('one', 1.916290731874155) \n",
            "\n",
            "*OUTPUT CORRESPONDING TO FIRST DOCUMENT\n",
            "\n",
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n",
            "\n",
            "\n",
            "\n",
            "                                     -----sklearn tdidfVectorizer Results-----\n",
            "\n",
            "\n",
            "*IDF VALUES [1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
            " 1.         1.91629073 1.        ]\n",
            "\n",
            "*FEATURE NAMES ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
            "\n",
            "*OUTPUT CORRESPONDING TO FIRST DOCUMENT\n",
            "\n",
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ycrKN74Aih3",
        "colab_type": "text"
      },
      "source": [
        "### **TASK 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3Gmvrj3JFCW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "676179b5-811a-4f6d-80e8-1d4cd38b9c7f"
      },
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "import math\n",
        "\n",
        "import pickle\n",
        "with open('cleaned_strings', 'rb') as f:\n",
        "    corpus_2 = pickle.load(f)\n",
        "\n",
        "documentsCount = len(corpus_2)\n",
        "\n",
        "q = set()\n",
        "vocab_2 = {}\n",
        "vocab_3 = {}\n",
        "idf_values_2 = {}\n",
        "\n",
        "def fit(dataset):\n",
        "    vocab_idf_values= {}\n",
        "    unique_words_2 = set() \n",
        "    if isinstance(dataset, (list,)):\n",
        "        for row in dataset: \n",
        "            review = dict(Counter(row.split()))                                          # for each review in the dataset\n",
        "            for word, freq in review.items():                                            # for each word in the review. \n",
        "                if len(word) < 2:\n",
        "                    continue\n",
        "                unique_words_2.add(word)\n",
        "    \n",
        "    for val in unique_words_2:\n",
        "        idf = compute_IDF(val,dataset)\n",
        "        if idf!=0:\n",
        "           vocab_idf_values[val]= idf      \n",
        "    idf_list = list(vocab_idf_values.values())\n",
        "    idf_list.sort(reverse=True)\n",
        "    \n",
        "    for s in range(0,50):\n",
        "        word = get_key(idf_list[s],vocab_idf_values)\n",
        "        #print(\"word = \", word , \"idf value = \", idf_list[s])\n",
        "        vocab_2.update({word:s})\n",
        "        vocab_3.update({word:idf_list[s]})\n",
        "    return vocab_3\n",
        "\n",
        "def get_key(val,my_dict): \n",
        "    for key, value in my_dict.items(): \n",
        "         if val == value:\n",
        "            my_dict.pop(key) \n",
        "            return key \n",
        "\n",
        "def compute_IDF(word,dataset):  \n",
        "    for val in q: \n",
        "        if word==val:\n",
        "           return 0\n",
        "    q.add(word)\n",
        "    z = 0 \n",
        "    if isinstance(dataset, (list,)):\n",
        "       for row in dataset:                        # for each review in the dataset\n",
        "           for word1 in row.split(\" \"):        \n",
        "               if word1 == word:\n",
        "                   z = z+1\n",
        "                   break\n",
        "               continue           \n",
        "    idf = 1 + math.log((1+documentsCount)/(1+z))\n",
        "    return idf\n",
        " \n",
        "idf_50 = fit (corpus_2)\n",
        "i = 1\n",
        "print(\"\\n\\n Top 50 words and their idf values\\n\")\n",
        "for s in idf_50.items():\n",
        "  print(i,\".\",s)\n",
        "  i = i+1\n",
        "\n",
        "rows_2 = []\n",
        "columns_2 = []\n",
        "values_2 = []\n",
        "idf_values_2 = {}\n",
        "k = set()\n",
        "\n",
        "def transform_2(dataset,vocab_2):\n",
        "    if isinstance(dataset, (list,)):\n",
        "        for idx, row in enumerate(tqdm(dataset)):              \n",
        "            review = dict(Counter(row.split()))    \n",
        "            for word, freq in review.items():                    \n",
        "                if len(word) < 2:\n",
        "                    continue\n",
        "                tf = freq/(len(review))\n",
        "                idf = computeIDF(word,vocab_2,dataset)\n",
        "                if idf!=0:\n",
        "                   idf_values_2[word]= idf\n",
        "                tfidf = tf * idf                                                                     \n",
        "                col_index = vocab_2.get(word, -1)                \n",
        "                if col_index !=-1:                             \n",
        "                   if tfidf!=0:\n",
        "                      rows_2.append(idx)\n",
        "                      columns_2.append(col_index)\n",
        "                      values_2.append(tfidf) \n",
        "        my_array = np.array(rows_2)\n",
        "        #print(\"\\nrows_2\", my_array)\n",
        "        return normalize(csr_matrix((values_2, (rows_2,columns_2)), shape=(len(dataset),len(vocab_2))))\n",
        "    else:\n",
        "        print(\"you need to pass list of strings\")\n",
        "\n",
        "\n",
        "def computeIDF(word,vocab,dataset):  \n",
        "    for val in k: \n",
        "        if word==val:\n",
        "           return 0\n",
        "    k.add(word)\n",
        "    z = 0 \n",
        "    if isinstance(dataset, (list,)):\n",
        "       for row in dataset:                        \n",
        "           for word1 in row.split(\" \"):        \n",
        "               if word1 == word:\n",
        "                   z = z+1\n",
        "                   break\n",
        "               continue\n",
        "    idf = 1 + math.log((1+documentsCount)/(1+z))\n",
        "    return idf\n",
        "\n",
        "y = transform_2(corpus_2,vocab_2)\n",
        "print(\"\\n\\nShape of Sparse Matrix\",y.shape)\n",
        "print(\"\\nShape of dense Matrix for each document\",y[0].shape)\n",
        "\n",
        "print(\"\\nFew dense representation are as follows: \")\n",
        "\n",
        "print(\"\\nDense Matrix for 11th document\")\n",
        "print(y[11].toarray())\n",
        "print(\"\\nDense Matrix for 19th document\")\n",
        "print(y[19].toarray())\n",
        "print(\"\\nDense Matrix for 350th document\")\n",
        "print(y[350].toarray())\n",
        "print(\"\\nDense Matrix for 644th document\")\n",
        "print(y[644].toarray())\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 18/746 [00:00<00:04, 166.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " Top 50 words and their idf values\n",
            "\n",
            "1 . ('modest', 6.922918004572872)\n",
            "2 . ('system', 6.922918004572872)\n",
            "3 . ('locations', 6.922918004572872)\n",
            "4 . ('impossible', 6.922918004572872)\n",
            "5 . ('dialogs', 6.922918004572872)\n",
            "6 . ('native', 6.922918004572872)\n",
            "7 . ('outside', 6.922918004572872)\n",
            "8 . ('trilogy', 6.922918004572872)\n",
            "9 . ('executed', 6.922918004572872)\n",
            "10 . ('critic', 6.922918004572872)\n",
            "11 . ('massive', 6.922918004572872)\n",
            "12 . ('syrupy', 6.922918004572872)\n",
            "13 . ('amaze', 6.922918004572872)\n",
            "14 . ('personally', 6.922918004572872)\n",
            "15 . ('highlights', 6.922918004572872)\n",
            "16 . ('owned', 6.922918004572872)\n",
            "17 . ('masculine', 6.922918004572872)\n",
            "18 . ('limitations', 6.922918004572872)\n",
            "19 . ('jerry', 6.922918004572872)\n",
            "20 . ('cheekbones', 6.922918004572872)\n",
            "21 . ('celebration', 6.922918004572872)\n",
            "22 . ('surroundings', 6.922918004572872)\n",
            "23 . ('rips', 6.922918004572872)\n",
            "24 . ('rita', 6.922918004572872)\n",
            "25 . ('propaganda', 6.922918004572872)\n",
            "26 . ('broke', 6.922918004572872)\n",
            "27 . ('tanks', 6.922918004572872)\n",
            "28 . ('monica', 6.922918004572872)\n",
            "29 . ('perplexing', 6.922918004572872)\n",
            "30 . ('limited', 6.922918004572872)\n",
            "31 . ('producer', 6.922918004572872)\n",
            "32 . ('awarded', 6.922918004572872)\n",
            "33 . ('donlevy', 6.922918004572872)\n",
            "34 . ('howdy', 6.922918004572872)\n",
            "35 . ('co', 6.922918004572872)\n",
            "36 . ('unfortunate', 6.922918004572872)\n",
            "37 . ('forgetting', 6.922918004572872)\n",
            "38 . ('blah', 6.922918004572872)\n",
            "39 . ('confusing', 6.922918004572872)\n",
            "40 . ('diving', 6.922918004572872)\n",
            "41 . ('existent', 6.922918004572872)\n",
            "42 . ('warn', 6.922918004572872)\n",
            "43 . ('dreary', 6.922918004572872)\n",
            "44 . ('biggest', 6.922918004572872)\n",
            "45 . ('ford', 6.922918004572872)\n",
            "46 . ('essence', 6.922918004572872)\n",
            "47 . ('fair', 6.922918004572872)\n",
            "48 . ('brooding', 6.922918004572872)\n",
            "49 . ('escalating', 6.922918004572872)\n",
            "50 . ('treachery', 6.922918004572872)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 746/746 [00:02<00:00, 278.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Shape of Sparse Matrix (746, 50)\n",
            "\n",
            "Shape of dense Matrix for each document (1, 50)\n",
            "\n",
            "Few dense representation are as follows: \n",
            "\n",
            "Dense Matrix for 11th document\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0.]]\n",
            "\n",
            "Dense Matrix for 19th document\n",
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.90453403 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.30151134 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.30151134 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.        ]]\n",
            "\n",
            "Dense Matrix for 350th document\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0.]]\n",
            "\n",
            "Dense Matrix for 644th document\n",
            "[[0.         0.         0.         0.70710678 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.70710678 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.        ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}